{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data preparation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#for string\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "#for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "#model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "#for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#check for null values\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of null values in 2 columns.let us drop the columns that are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['keyword','location','id'],inplace=True,axis=1)\n",
    "test.drop(['location','keyword','id'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
       "3259  Storm in RI worse than last hurricane. My city...\n",
       "3260  Green Line derailment in Chicago http://t.co/U...\n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...\n",
       "3262  #CityofCalgary has activated its Municipal Eme..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop=stopwords.words('english')\n",
    "stop[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(df):\n",
    "    df['cleaned_text']=df['text'].apply(lambda x:' '.join(x.lower() for x in x.split()))\n",
    "\n",
    "def remove_links(df):\n",
    "    df['cleaned_text']=df['cleaned_text'].apply(lambda x:' '.join([x for x in x.split() if x[:3] != 'http']))\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    df['cleaned_text']=df['cleaned_text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    df['cleaned_text']=df['cleaned_text'].apply(lambda x:' '.join([x for x in x.split() if x not in stop]))\n",
    "\n",
    "def lemmatization(df):\n",
    "    lemm=WordNetLemmatizer()\n",
    "    df['cleaned_text']=df['cleaned_text'].apply(lambda x:' '.join([lemm.lemmatize(x) for x in x.split()]))\n",
    "    return (df[['text','cleaned_text']].head())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1             Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are ...   \n",
       "3  13,000 people receive #wildfires evacuation or...   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident asked shelter place notified officer ...  \n",
       "3  13000 people receive wildfire evacuation order...  \n",
       "4  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transforming training data\n",
    "lowercasing(train)\n",
    "remove_links(train)\n",
    "remove_punctuation(train)\n",
    "remove_stopwords(train)\n",
    "lemmatization(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond goose fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                 Just happened a terrible car crash   \n",
       "1  Heard about #earthquake is different cities, s...   \n",
       "2  there is a forest fire at spot pond, geese are...   \n",
       "3           Apocalypse lighting. #Spokane #wildfires   \n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                        happened terrible car crash  \n",
       "1  heard earthquake different city stay safe ever...  \n",
       "2  forest fire spot pond goose fleeing across str...  \n",
       "3               apocalypse lighting spokane wildfire  \n",
       "4              typhoon soudelor kill 28 china taiwan  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transforming test data\n",
    "lowercasing(test)\n",
    "remove_links(test)\n",
    "remove_punctuation(test)\n",
    "remove_stopwords(test)\n",
    "lemmatization(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countvectorizer\n",
    "cv=CountVectorizer()\n",
    "X_train_cv=cv.fit_transform(train['cleaned_text'])\n",
    "X_test_cv=cv.transform(test['cleaned_text'])\n",
    "\n",
    "y_train=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vectorizer\n",
    "tf=TfidfVectorizer()\n",
    "X_train_tf=cv.fit_transform(train['cleaned_text'])\n",
    "X_test_tf=cv.transform(test['cleaned_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression()\n",
    "clf2=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.9655851832391961\n"
     ]
    }
   ],
   "source": [
    "#fitting on countvectorizer data\n",
    "clf.fit(X_train_cv,y_train)\n",
    "predicts_cv=clf.predict(X_test_cv)\n",
    "print('Score on training data:',clf.score(X_train_cv,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.9655851832391961\n"
     ]
    }
   ],
   "source": [
    "#fitting on tfidf vectorizer\n",
    "clf2.fit(X_train_tf,y_train)\n",
    "predicts_tf=clf2.predict(X_test_tf)\n",
    "print('Score on training data:',clf2.score(X_train_tf,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.9655851832391961\n"
     ]
    }
   ],
   "source": [
    "#countvectorizer classificaiton with C value\n",
    "clf3=LogisticRegression(C=1)\n",
    "clf3.fit(X_train_cv,y_train)\n",
    "predicts_cv=clf3.predict(X_test_cv)\n",
    "print('Score on training data:',clf3.score(X_train_cv,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train['cleaned_text'])\n",
    "train_matrix=tokenizer.texts_to_sequences(train['cleaned_text'])\n",
    "test_matrix=tokenizer.texts_to_sequences(test['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the maxlen of tweet\n",
    "train['length']=train['cleaned_text'].apply(lambda x:len([x for x in x.split()]))\n",
    "train['length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=25\n",
    "train_matrix=pad_sequences(train_matrix,padding='post',maxlen=maxlen,truncating='post')\n",
    "test_matrix=pad_sequences(test_matrix,padding='post',maxlen=maxlen,truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 25), (3263, 25))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape,test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.Sequential([\n",
    "    layers.Embedding(10000,20,input_length=maxlen),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32,activation='relu'),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 20)            200000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                16032     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 216,065\n",
      "Trainable params: 216,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/12\n",
      "5329/5329 [==============================] - 1s 177us/sample - loss: 0.6409 - accuracy: 0.6425 - val_loss: 0.6018 - val_accuracy: 0.6983\n",
      "Epoch 2/12\n",
      "5329/5329 [==============================] - 0s 70us/sample - loss: 0.4424 - accuracy: 0.8195 - val_loss: 0.5136 - val_accuracy: 0.7469\n",
      "Epoch 3/12\n",
      "5329/5329 [==============================] - 0s 81us/sample - loss: 0.3213 - accuracy: 0.8743 - val_loss: 0.5214 - val_accuracy: 0.7478\n",
      "Epoch 4/12\n",
      "5329/5329 [==============================] - 0s 78us/sample - loss: 0.2537 - accuracy: 0.9073 - val_loss: 0.5354 - val_accuracy: 0.7439\n",
      "Epoch 5/12\n",
      "5329/5329 [==============================] - 0s 79us/sample - loss: 0.1945 - accuracy: 0.9338 - val_loss: 0.5841 - val_accuracy: 0.7290\n",
      "Epoch 6/12\n",
      "5329/5329 [==============================] - 0s 73us/sample - loss: 0.1509 - accuracy: 0.9550 - val_loss: 0.6276 - val_accuracy: 0.7316\n",
      "Epoch 7/12\n",
      "5329/5329 [==============================] - 0s 65us/sample - loss: 0.1242 - accuracy: 0.9623 - val_loss: 0.6833 - val_accuracy: 0.7062\n",
      "Epoch 8/12\n",
      "5329/5329 [==============================] - 0s 65us/sample - loss: 0.1065 - accuracy: 0.9694 - val_loss: 0.7765 - val_accuracy: 0.6673\n",
      "Epoch 9/12\n",
      "5329/5329 [==============================] - 0s 65us/sample - loss: 0.0936 - accuracy: 0.9724 - val_loss: 0.7481 - val_accuracy: 0.6983\n",
      "Epoch 10/12\n",
      "5329/5329 [==============================] - 0s 66us/sample - loss: 0.0843 - accuracy: 0.9760 - val_loss: 0.7647 - val_accuracy: 0.6940\n",
      "Epoch 11/12\n",
      "5329/5329 [==============================] - 0s 66us/sample - loss: 0.0784 - accuracy: 0.9769 - val_loss: 0.8846 - val_accuracy: 0.6515\n",
      "Epoch 12/12\n",
      "5329/5329 [==============================] - 0s 65us/sample - loss: 0.0731 - accuracy: 0.9790 - val_loss: 0.9150 - val_accuracy: 0.6370\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(train_matrix,y_train,epochs=12,verbose=1,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=models.Sequential([\n",
    "    layers.Embedding(10000,20,input_length=maxlen),\n",
    "    layers.Bidirectional(layers.LSTM(16)),\n",
    "    layers.Dense(32,activation='relu'),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/8\n",
      "5329/5329 [==============================] - 6s 1ms/sample - loss: 0.6139 - accuracy: 0.6566 - val_loss: 0.5367 - val_accuracy: 0.7430\n",
      "Epoch 2/8\n",
      "5329/5329 [==============================] - 3s 485us/sample - loss: 0.4090 - accuracy: 0.8292 - val_loss: 0.4829 - val_accuracy: 0.7833\n",
      "Epoch 3/8\n",
      "5329/5329 [==============================] - 3s 503us/sample - loss: 0.3432 - accuracy: 0.8594 - val_loss: 0.4855 - val_accuracy: 0.7863\n",
      "Epoch 4/8\n",
      "5329/5329 [==============================] - 3s 504us/sample - loss: 0.3068 - accuracy: 0.8784 - val_loss: 0.4919 - val_accuracy: 0.7706\n",
      "Epoch 5/8\n",
      "5329/5329 [==============================] - 3s 518us/sample - loss: 0.2818 - accuracy: 0.8906 - val_loss: 0.5064 - val_accuracy: 0.7623\n",
      "Epoch 6/8\n",
      "5329/5329 [==============================] - 3s 491us/sample - loss: 0.2618 - accuracy: 0.8981 - val_loss: 0.5203 - val_accuracy: 0.7688\n",
      "Epoch 7/8\n",
      "5329/5329 [==============================] - 3s 490us/sample - loss: 0.2447 - accuracy: 0.9050 - val_loss: 0.5680 - val_accuracy: 0.7360\n",
      "Epoch 8/8\n",
      "5329/5329 [==============================] - 3s 503us/sample - loss: 0.2281 - accuracy: 0.9126 - val_loss: 0.5604 - val_accuracy: 0.7706\n"
     ]
    }
   ],
   "source": [
    "hist2=model2.fit(train_matrix,y_train,epochs=8,verbose=1,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=models.Sequential([\n",
    "    layers.Embedding(10000,20,input_length=maxlen),\n",
    "    layers.Bidirectional(layers.LSTM(16)),\n",
    "    layers.Dense(32,activation='relu'),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/8\n",
      "5329/5329 [==============================] - 6s 1ms/sample - loss: 0.6159 - accuracy: 0.6620 - val_loss: 0.5137 - val_accuracy: 0.7461\n",
      "Epoch 2/8\n",
      "5329/5329 [==============================] - 3s 549us/sample - loss: 0.3468 - accuracy: 0.8536 - val_loss: 0.5196 - val_accuracy: 0.7658\n",
      "Epoch 3/8\n",
      "5329/5329 [==============================] - 3s 540us/sample - loss: 0.2246 - accuracy: 0.9107 - val_loss: 0.5520 - val_accuracy: 0.7417\n",
      "Epoch 4/8\n",
      "5329/5329 [==============================] - 3s 507us/sample - loss: 0.1565 - accuracy: 0.9424 - val_loss: 0.7027 - val_accuracy: 0.7198\n",
      "Epoch 5/8\n",
      "5329/5329 [==============================] - 3s 503us/sample - loss: 0.1172 - accuracy: 0.9612 - val_loss: 0.9481 - val_accuracy: 0.6953\n",
      "Epoch 6/8\n",
      "5329/5329 [==============================] - 3s 537us/sample - loss: 0.0968 - accuracy: 0.9683 - val_loss: 0.9574 - val_accuracy: 0.6799\n",
      "Epoch 7/8\n",
      "5329/5329 [==============================] - 3s 510us/sample - loss: 0.0814 - accuracy: 0.9713 - val_loss: 0.9950 - val_accuracy: 0.7040\n",
      "Epoch 8/8\n",
      "5329/5329 [==============================] - 3s 503us/sample - loss: 0.0712 - accuracy: 0.9762 - val_loss: 0.9871 - val_accuracy: 0.7071\n"
     ]
    }
   ],
   "source": [
    "hist3=model3.fit(train_matrix,y_train,epochs=8,verbose=1,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
